{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Quality Summary\n",
    "A highly imbalanced dataset, with three of the 7 classes being so rare as to be outliers.  \n",
    "After seeing these results, I have become uncertain as to how well these data *can* actually predict the target variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Logistic Regression\n",
    "OOB Accuracy Score: 0.518462  \n",
    "OOB Classification Report:  \n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          3       0.00      0.00      0.00        12\n",
    "          4       0.50      0.02      0.03        64\n",
    "          5       0.56      0.57      0.57       643\n",
    "          6       0.50      0.72      0.59       845\n",
    "          7       0.48      0.10      0.17       329\n",
    "          8       0.00      0.00      0.00        54\n",
    "          9       0.00      0.00      0.00         3\n",
    "\n",
    "avg / total       0.50      0.52      0.47      1950\n",
    "\n",
    "A prediction accuracy little better than random, with an F1 score to match. It also failed to accurately predict any of the outlier classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random Forest\n",
    "OOB Accuracy Score: 0.628718  \n",
    "OOB Classification Report:  \n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          3       0.00      0.00      0.00        12\n",
    "          4       0.27      0.09      0.14        64\n",
    "          5       0.65      0.69      0.67       643\n",
    "          6       0.61      0.71      0.66       845\n",
    "          7       0.68      0.48      0.56       329\n",
    "          8       0.79      0.28      0.41        54\n",
    "          9       0.00      0.00      0.00         3\n",
    "\n",
    "avg / total       0.62      0.63      0.62      1950\n",
    "\n",
    "The best prediction accuracy of any of the models; 62.9%. It even managed to get some of the samples from one of the outlier classes correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### K Nearest Neighbours \n",
    "OOB Accuracy Score: 0.542051  \n",
    "OOB Classification Report:  \n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          3       0.00      0.00      0.00        12\n",
    "          4       0.22      0.08      0.11        64\n",
    "          5       0.56      0.60      0.58       643\n",
    "          6       0.55      0.62      0.58       845\n",
    "          7       0.51      0.43      0.47       329\n",
    "          8       0.29      0.07      0.12        54\n",
    "          9       0.00      0.00      0.00         3\n",
    "\n",
    "avg / total       0.52      0.54      0.53      1950\n",
    "\n",
    "An accuracy not hugely better than random guessing, and an even lower F1 score, as it incorrectly predicted many of class 4's members. It did manage to get a low proportoin of the outlier class 8 correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Multi-layer Perceptron - 200 iterations\n",
    "OOB Accuracy Score: 0.557949  \n",
    "OOB Classification Report:  \n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          3       0.00      0.00      0.00        12\n",
    "          4       0.36      0.08      0.13        64\n",
    "          5       0.59      0.61      0.60       643\n",
    "          6       0.55      0.64      0.59       845\n",
    "          7       0.54      0.44      0.48       329\n",
    "          8       0.25      0.02      0.03        54\n",
    "          9       0.00      0.00      0.00         3\n",
    "\n",
    "avg / total       0.54      0.56      0.54      1950\n",
    "\n",
    "A simple feed-forward MLP did not produce notable results. Similarly to KNN, it was poor on class 4, but managed to get a small proportion of class 8 correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Multi-layer Perceptron - 500 iterations\n",
    "OOB Accuracy Score: 0.559487  \n",
    "OOB Classification Report:  \n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          3       0.00      0.00      0.00        12\n",
    "          4       0.33      0.06      0.11        64\n",
    "          5       0.59      0.61      0.60       643\n",
    "          6       0.54      0.66      0.60       845\n",
    "          7       0.56      0.40      0.46       329\n",
    "          8       0.50      0.06      0.10        54\n",
    "          9       0.00      0.00      0.00         3\n",
    "\n",
    "avg / total       0.55      0.56      0.54      1950\n",
    "\n",
    "The same MLP with an interation limit 2.5 times higher got almost the same level of accuracy, and marginally better precision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Support Vector Machine\n",
    "OOB Accuracy Score: 0.547179  \n",
    "OOB Classification Report:  \n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          3       0.00      0.00      0.00        12\n",
    "          4       0.00      0.00      0.00        64\n",
    "          5       0.60      0.58      0.59       643\n",
    "          6       0.52      0.75      0.61       845\n",
    "          7       0.56      0.19      0.29       329\n",
    "          8       0.00      0.00      0.00        54\n",
    "          9       0.00      0.00      0.00         3\n",
    "\n",
    "avg / total       0.52      0.55      0.51      1950\n",
    "\n",
    "Accuracy is again nearly as bad as random guesswork. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Decision Tree\n",
    "OOB Accuracy Score: 0.584615  \n",
    "OOB Classification Report:  \n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          3       0.00      0.00      0.00        12\n",
    "          4       0.19      0.17      0.18        64\n",
    "          5       0.63      0.64      0.63       643\n",
    "          6       0.61      0.63      0.62       845\n",
    "          7       0.54      0.51      0.52       329\n",
    "          8       0.34      0.43      0.38        54\n",
    "          9       0.00      0.00      0.00         3\n",
    "\n",
    "avg / total       0.58      0.58      0.58      1950\n",
    "\n",
    "Remarkably high scores, but only when compared to the performance of most other models here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Naive Bayes\n",
    "OOB Accuracy Score: 0.441026  \n",
    "OOB Classification Report:  \n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          3       0.08      0.08      0.08        12\n",
    "          4       0.24      0.12      0.16        64\n",
    "          5       0.48      0.43      0.45       643\n",
    "          6       0.46      0.50      0.48       845\n",
    "          7       0.41      0.44      0.42       329\n",
    "          8       0.13      0.13      0.13        54\n",
    "          9       0.00      0.00      0.00         3\n",
    "\n",
    "avg / total       0.44      0.44      0.44      1950\n",
    "\n",
    "The only model to get worse performance than random guessing, at 44% accuracy. It was, however, the only model to correctly predict a single instance of class 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "I think that the utter failure to achieve even 70% accuracy by any classifier is suggestive of this dataset not being sufficient to predict the target variable. \n",
    "\n",
    "Either a much greater amount of data is required, which cannot be obtained since I cannot find any other similar wine datasets from Portugal from 2009, or there isn't much of a connection between the features and the target.  \n",
    "Given the lack of correlation between anything and the target in the EDA, I am inclined to favour the second possibility. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
