{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Summary\n",
    "A balanced, dense dataset, with three classes and not many outliers.  \n",
    "Two of the classes are not cleanly linearly separable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Logistic Regression\n",
    "OOB Accuracy Score: **0.933333**  \n",
    "OOB Classification Report:  \n",
    "\n",
    "|Class|Precision|Recall|F1-score|Support|\n",
    "|---|---|---|---|---|\n",
    "|0|1.00|0.95|0.98|21|\n",
    "|1|0.82|0.90|0.86|10|\n",
    "|2|0.93|0.93|0.93|14|\n",
    "\n",
    "|Average|Precision|Recall|F1-score|Support|\n",
    "|---|---|---|---|---|\n",
    "|micro avg|0.93|0.93|0.93|45|\n",
    "|macro avg|0.92|0.93|0.92|45|\n",
    "|weighted|avg|0.94|0.93|0.93|45|\n",
    "\n",
    "GridSearchCV best score: **0.942857**  \n",
    "GridSearchCV best estimator: LogisticRegression(C=30, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
    "          n_jobs=None, penalty='l2', random_state=4, solver='warn',\n",
    "          tol=3e-06, verbose=0, warm_start=False)\n",
    "\n",
    "Although 93% is the lowest score, logistic regression performed surprisingly well on a linearly inseparable dataset. A meaninful improvement of 1% was found by grid search, by reducing regularisation massively, and decreasing the tolerance for the stopping criteria. \n",
    "\n",
    "It is worth noting that despite the high overall accuracy, the f1-score for the second class was only 0.86, with the precision being 0.82. The linear separability of class 0 from the other two resulted in very high scores, that when averaged hide the weakness of this model in predicting the linearly inseparable classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random Forest\n",
    "OOB Accuracy Score: **0.955556**  \n",
    "OOB Classification Report:  \n",
    "\n",
    "|Class|Precision|Recall|F1-score|Support|\n",
    "|---|---|---|---|---|\n",
    "|0|1.00|1.00|1.00|21|\n",
    "|1|0.90|0.90|0.90|10|\n",
    "|2|0.93|0.93|0.93|14|\n",
    "\n",
    "|Average|Precision|Recall|F1-score|Support|\n",
    "|---|---|---|---|---|\n",
    "|micro avg|0.96|0.96|0.96|45|\n",
    "|macro avg|0.94|0.94|0.94|45|\n",
    "|weighted|avg|0.96|0.96|0.96|45|\n",
    "\n",
    "GridSearchCV best score: **0.952381**  \n",
    "GridSearchCV best estimator: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=None,\n",
    "            oob_score=False, random_state=4, verbose=0, warm_start=False)\n",
    "\n",
    "Random forrest got 95-96% accuracy overall, including after grid search. Again the average f1-score is pulled up by a perfect f1-score for class 0, somewhat hiding the lower f1-scores from other classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### K Nearest Neighbors\n",
    "OOB Accuracy Score: **0.955556**  \n",
    "OOB Classification Report:  \n",
    "\n",
    "|Class|Precision|Recall|F1-score|Support|\n",
    "|---|---|---|---|---|\n",
    "|0|1.00|1.00|1.00|21|\n",
    "|1|0.90|0.90|0.90|10|\n",
    "|2|0.93|0.93|0.93|14|\n",
    "\n",
    "|Average|Precision|Recall|F1-score|Support|\n",
    "|---|---|---|---|---|\n",
    "|micro avg|0.96|0.96|0.96|45|\n",
    "|macro avg|0.94|0.94|0.94|45|\n",
    "|weighted|avg|0.96|0.96|0.96|45|\n",
    "\n",
    "GridSearchCV best score: **0.971429**  \n",
    "GridSearchCV best estimator: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_jobs=None, n_neighbors=4, p=2,\n",
    "           weights='uniform')\n",
    "\n",
    "The increase of 1.5% between the OOB 95.5% and grid searched 97% accuracy scores of kNN is the reduction in counted neighbours from five to four. kNN results appear to suffer from the same issue as the above classifiers, in that the perfect score of class 0 hides the 0.90-0.93 of classes 1 and 2. \n",
    "\n",
    "Overall, not a bad score, though it does appear obvious in hindsight that with so much overlap in feature space, a feature space-distance based classification algorithm would not be able to get to the highest scores of accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Multi-layer Perceptron - 200 iterations\n",
    "OOB Accuracy Score: **0.933333**  \n",
    "OOB Classification Report:  \n",
    "\n",
    "|Class|Precision|Recall|F1-score|Support|\n",
    "|---|---|---|---|---|\n",
    "|0|1.00|0.95|0.98|21|\n",
    "|1|0.82|0.90|0.86|10|\n",
    "|2|0.93|0.93|0.93|14|\n",
    "\n",
    "|Average|Precision|Recall|F1-score|Support|\n",
    "|---|---|---|---|---|\n",
    "|micro avg|0.93|0.93|0.93|45|\n",
    "|macro avg|0.92|0.93|0.92|45|\n",
    "|weighted|avg|0.94|0.93|0.93|45|\n",
    "\n",
    "GridSearchCV best score: **0.971429**  \n",
    "GridSearchCV best estimator: MLPClassifier(activation='logistic', alpha=3e-06, batch_size='auto',\n",
    "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
    "       random_state=4, shuffle=True, solver='adam', tol=0.0001,\n",
    "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
    "\n",
    "\n",
    "Start time: 12:37:00  \n",
    "End time: 12:39:12  \n",
    "\n",
    "Expectably, even with 100 nodes, a single layer neural network only achieved results equivalent to logistic regression, because it _is_ logistic regression. \n",
    "\n",
    "What's interesting, is what happens when multiple layers are used. The accuracy jumps by 4% up to 97%. What is also interesting is that this score is _exactly the same_ as the score achieved by the best kNN model, and is the _second highest_ score achieved by the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Multi-layer Perceptron - 500 iterations\n",
    "OOB Accuracy Score: **0.933333**  \n",
    "OOB Classification Report:  \n",
    "\n",
    "|Class|Precision|Recall|F1-score|Support|\n",
    "|---|---|---|---|---|\n",
    "|0|1.00|0.95|0.98|21|\n",
    "|1|0.82|0.90|0.86|10|\n",
    "|2|0.93|0.93|0.93|14|\n",
    "\n",
    "|Average|Precision|Recall|F1-score|Support|\n",
    "|---|---|---|---|---|\n",
    "|micro avg|0.93|0.93|0.93|45|\n",
    "|macro avg|0.92|0.93|0.92|45|\n",
    "|weighted|avg|0.94|0.93|0.93|45|\n",
    "\n",
    "GridSearchCV best score: **0.971429**  \n",
    "GridSearchCV best estimator: MLPClassifier(activation='logistic', alpha=3e-06, batch_size='auto',\n",
    "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
    "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
    "       random_state=4, shuffle=True, solver='adam', tol=0.0001,\n",
    "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
    "\n",
    "\n",
    "Start time: 12:39:12  \n",
    "End time: 12:42:57  \n",
    "\n",
    "Again, the accuracy score of a single layer of perceptrons is exactly equivalent to logistic regression, which is basically the same as, just repeated 100 times. \n",
    "\n",
    "What is confusing, is that the best estimator - which achieves 97% accuracy - only has one layer of perceptrons. Conventional wisdom with neural networks is that more layers = more accuracy, but here it appears that more iterations of multiple repetitions of logistic regression = more accuracy. The extra layers in the MLP-200 _weren't necessary_, just more training time. \n",
    "\n",
    "An interesting future investigation would be finding out about the relationship between the computational expense of more layers vs. more training iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Support Vector Machine\n",
    "OOB Accuracy Score: **0.955556**  \n",
    "OOB Classification Report:  \n",
    "\n",
    "|Class|Precision|Recall|F1-score|Support|\n",
    "|---|---|---|---|---|\n",
    "|0|1.00|1.00|1.00|21|\n",
    "|1|0.90|0.90|0.90|10|\n",
    "|2|0.93|0.93|0.93|14|\n",
    "\n",
    "|Average|Precision|Recall|F1-score|Support|\n",
    "|---|---|---|---|---|\n",
    "|micro avg|0.96|0.96|0.96|45|\n",
    "|macro avg|0.94|0.94|0.94|45|\n",
    "|weighted|avg|0.96|0.96|0.96|45|\n",
    "\n",
    "GridSearchCV best score: **0.971429**  \n",
    "GridSearchCV best estimator: SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
    "  kernel='rbf', max_iter=-1, probability=False, random_state=4,\n",
    "  shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "The support vector classifier got the exact same accuracy scores as kNN, which are good. Like all the preceeding classifiers, it has excellent prediction of class 0, and ~10% less accuracy on classes 1 and 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Decision Tree\n",
    "OOB Accuracy Score: **0.977778**  \n",
    "OOB Classification Report:  \n",
    "\n",
    "|Class|Precision|Recall|F1-score|Support|\n",
    "|---|---|---|---|---|\n",
    "|0|1.00|1.00|1.00|21|\n",
    "|1|1.00|0.90|0.95|10|\n",
    "|2|0.93|1.00|0.97|14|\n",
    "\n",
    "|Average|Precision|Recall|F1-score|Support|\n",
    "|---|---|---|---|---|\n",
    "|micro avg|0.98|0.98|0.98|45|\n",
    "|macro avg|0.98|0.97|0.97|45|\n",
    "|weighted|avg|0.98|0.98|0.98|45|\n",
    "\n",
    "GridSearchCV best score: **0.952381**  \n",
    "GridSearchCV best estimator: DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=4,\n",
    "            splitter='best')\n",
    "\n",
    "The decision tree achieved one of the two instances of the highest score among all the classifiers. At 98%, it can be considered highly accurate.  \n",
    "Interestingly, it's identification of classes 1 and 2 appear to be inversely accurate, with predictions of class 1 always being accurate, but class 1 not always being accurately predicted, whereas not everything predicted as class 2 is actually class 2, but everything that is class 2 was correctly identified as class 2.  \n",
    "Basically, it got all of class 2 correct, but thought 10% of class 1 were also class 2. \n",
    "\n",
    "Oddly, the grid search got a lower accuracy score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Naive Bayes\n",
    "OOB Accuracy Score: **0.977778**  \n",
    "OOB Classification Report:  \n",
    "\n",
    "|Class|Precision|Recall|F1-score|Support|\n",
    "|---|---|---|---|---|\n",
    "|0|1.00|1.00|1.00|21|\n",
    "|1|0.91|1.00|0.95|10|\n",
    "|2|1.00|0.93|0.96|14|\n",
    "\n",
    "|Average|Precision|Recall|F1-score|Support|\n",
    "|---|---|---|---|---|\n",
    "|micro avg|0.98|0.98|0.98|45|\n",
    "|macro avg|0.97|0.98|0.97|45|\n",
    "|weighted|avg|0.98|0.98|0.98|45|\n",
    "\n",
    "The other instance of the highest score among all the classifiers, naive bayes achieved 98% accuracy. Similarly to the decision tree, it appears to have correctly classified the entirety of one of the linearly inseperable classes, and misclassified some of the other class, but the other way around from the decision tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Since all classifiers scored >90%, they are all clearly capable of dealing with some linear inseparability to some degree. Obviously some are better than others, though the best were still just learning to misclassify one class in order to get a perfect score on the other. \n",
    "\n",
    "An upper bound of 97.1%-97.8% appears to exist on classifying this dataset (at least when using lightly engineered models). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
